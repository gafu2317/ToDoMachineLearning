# タスクスケジューリング実験設計書

## 実験の目的

集中力の変動を考慮した強化学習によるタスクスケジューリングが、
従来の単純なルールベース手法（期限順、重要度順）よりも効率的に
タスクを完了できるかを検証する。

## 仮説

自分で考えた単純な優先順位（期限順や重要度順）よりも、
集中力の変動などを考慮した強化学習によるスケジューリングの方が
効率的にタスクを完了できる。

## 実験設計

### データセット構成

#### 学習用データセット（Training Set）
- **セット数**: 8,000セット
- **用途**: 強化学習モデルの学習のみ
- **保存場所**: `dataset/train/`

#### テスト用データセット（Test Set）
- **セット数**: 2,000セット
- **用途**: 全スケジューラの性能評価のみ
- **保存場所**: `dataset/test/`

**重要な原則:**
- 学習用とテスト用のデータセットは完全に分離
- テストセットは学習に一切使用しない
- これにより過学習（オーバーフィッティング）の検出が可能

### タスクセットの仕様

各セットは以下の条件で生成されたタスクを含む：

#### 基本設定
- **タスク数**: 60個（固定）
- **シミュレーション期間**: 7日間
- **1日の作業時間**: 8時間（480分）
- **総作業可能時間**: 56時間（3,360分）

#### タスク属性

**1. 所要時間（base_duration_minutes）**
- 短時間タスク（70%）: 15-60分
- 中時間タスク（20%）: 60-120分
- 長時間タスク（10%）: 120-180分

**2. 重要度（priority）**
- LOW（60%）: スコア倍率 1倍
- MEDIUM（25%）: スコア倍率 2倍
- HIGH（15%）: スコア倍率 3倍

**3. 締切（deadline）**
- シミュレーション開始から7日以内にランダムに設定
- 各タスクに個別の締切が設定される

**4. ジャンル（genre）**
- 4種類: "1", "2", "3", "4"
- 各ジャンル25%の確率で割り当て
- ジャンル切り替えによる集中力への影響あり

**5. スコア計算式**
```
スコア = 所要時間（分） × 重要度倍率
```

例:
- 60分のLOWタスク: 60 × 1 = 60点
- 120分のMEDIUMタスク: 120 × 2 = 240点
- 180分のHIGHタスク: 180 × 3 = 540点

### 集中力モデル

#### 基本パラメータ
- **初期集中力**: 1.0（100%）
- **最低集中力**: 0.2（20%）
- **連続作業可能時間**: 120分（基準値）
- **休憩時間**: 15分
- **減衰モデル**: 指数減衰

#### 減衰式
```
集中力 = 初期レベル × e^(-decay_factor × 連続作業時間 / 120)
```

#### 個人差（concentration_sustainability）
- **short**: decay_factor = 1.3（疲れやすい）
- **medium**: decay_factor = 1.0（標準）
- **long**: decay_factor = 0.8（疲れにくい）

#### 作業効率への影響
- 集中力 ≥ 0.7: 効率 0.8倍（20%時短）
- 0.4 ≤ 集中力 < 0.7: 効率 1.0倍（通常）
- 集中力 < 0.4: 効率 1.2倍（20%遅延）

#### ジャンル切り替え効果
個人設定により2タイプ：
- **同ジャンル継続タイプ**: 同じジャンル継続で+5%、切り替えでペナルティなし
- **ジャンル切り替えタイプ**: ジャンル切り替えで+5%、継続でペナルティなし

### 比較するスケジューラ

#### 1. Deadline Scheduler（期限順）
- **戦略**: 締切が近いタスクから優先的に実行
- **休憩**: 集中力が0.4以下で休憩
- **特徴**: シンプルで直感的

#### 2. Priority Scheduler（重要度順）
- **戦略**: 重要度が高いタスクから優先的に実行
- **休憩**: 集中力が0.4以下で休憩
- **特徴**: スコア最大化を狙う

#### 3. Random Scheduler（ランダム）
- **戦略**: 残りタスクからランダムに選択
- **休憩**: 集中力が0.4以下で休憩
- **特徴**: ベースライン（下限性能の確認）

#### 4. RL Scheduler（強化学習）
- **戦略**: Q-learningによる最適タスク選択
- **状態空間**: タスク数、平均重要度、平均時間、最短締切、集中力
- **行動空間**: どのタスクを実行するか
- **報酬関数**:
  - 基本完了報酬: スコア × 0.1
  - 締切遵守ボーナス: 早期完了で最大+20
  - 締切違反ペナルティ: 遅延で最大-50
  - 高集中完了ボーナス: 集中力0.7以上で+5
- **学習設定**:
  - 学習率: 0.1
  - 割引率: 0.9
  - 探索率（学習時）: 0.5 → 0.1
  - 探索率（テスト時）: 0.05

### 評価指標

#### 1. 総スコア（Primary Metric）
```
総スコア = Σ(完了タスクの所要時間 × 重要度)
```
- 高いほど重要なタスクを多く完了

#### 2. 完了率（Completion Rate）
```
完了率 = 完了タスク数 / 総タスク数
```
- 0.0～1.0の範囲
- 1.0に近いほど多くのタスクを完了

#### 3. 締切遵守率（Deadline Compliance Rate）
```
締切遵守率 = 締切内完了タスク数 / 完了タスク数
```
- 0.0～1.0の範囲
- 1.0に近いほど締切を守っている

#### 4. 効率（Efficiency）
```
効率 = 総スコア / 使用時間
```
- 高いほど短時間で高スコアを達成

### 実験手順

#### フェーズ1: データセット生成
```bash
python generate_task_dataset.py
```
- 学習用8,000セット、テスト用2,000セットを生成
- 各セット60タスク、合計60万タスク
- 生成時間: 約100秒
- ディスク使用量: 約200MB

#### フェーズ2: 強化学習モデルの学習
```bash
python train_rl_model.py
```
- **学習データ**: trainデータセットのみ使用
- **エピソード数**: 1,000回
- **データローテーション**: 8,000セットを循環利用
- **学習時間**: 数分～10分程度
- **出力**: 学習済みモデル（trained_models/rl_model_default.pkl）

#### フェーズ3: 性能評価実験
```bash
python run_full_experiment.py
```
- **評価データ**: testデータセットのみ使用
- **実験回数**: 50回（デフォルト）
- **評価対象**: 全4スケジューラ
- **データローテーション**: 2,000セットから順次使用
- **出力**:
  - 詳細データ（CSV）
  - 統計レポート（Markdown）
  - 強化学習分析レポート

### 統計的検定

#### 使用する検定手法
- **Welchのt検定**: 2群の平均値差を検定
- **有意水準**: p < 0.05

#### 比較ペア
- RL vs Deadline
- RL vs Priority
- RL vs Random
- Priority vs Deadline

### 期待される結果

#### 仮説が正しい場合
- RL Schedulerが総スコアで最高値を示す
- 統計的に有意な差（p < 0.05）が確認される
- 特に集中力管理が重要な場面で差が顕著

#### 考慮すべき点
- 過学習の兆候（train/testスコア差）
- 学習曲線の収束状況
- 計算コストと性能向上のトレードオフ

## データセットの再現性

### ランダムシード
- 現状: シードを固定していない
- 各生成で異なるデータセット
- 再現が必要な場合は、既存のデータセットを保持

### バージョン管理
- データセットは `.gitignore` で除外
- 各自でローカル生成
- 必要に応じてデータセットを共有（外部ストレージ）

## 実験環境

### システム要件
- Python 3.8以上
- 必要なライブラリ: numpy, pandas, matplotlib など
- ディスク空き容量: 500MB以上推奨

### 実行環境の記録
実験実行時に以下を記録：
- Pythonバージョン
- 依存ライブラリバージョン
- OS情報
- 実行日時

## 参考文献・関連研究

（必要に応じて追加）

---

**文書バージョン**: 1.0
**最終更新日**: 2026-01-25
**作成者**: （プロジェクトメンバー名）
