# ToDoMachineLearning

強化学習によるタスクスケジューリングと単純ルールベース手法の比較実験。

## 仮説

期限順や重要度順のルールベース手法よりも、集中力の変動を考慮した強化学習スケジューラーの方がタスク完了スコアが高まるか。

## 比較対象（4手法）

| スケジューラー | 戦略 |
|---|---|
| **Deadline** | 期限が近いタスクを優先 |
| **Priority** | 重要度が高いタスクを優先 |
| **Random** | 残りタスクからランダム選択 |
| **RL (Q-learning)** | 集中力・残りタスク・経過時間を状態とする強化学習 |

すべて「集中力が閾値以下になったら休憩」という共通の休憩戦略を持つ。

## シミュレーション設定

| パラメータ | 値 |
|---|---|
| シミュレーション期間 | 7 日間 |
| 1 日の作業時間 | 8 時間（480 分） |
| タスク数 | 60 個 |
| 実験回数 | 50 回 |

## コンセプト

### タスク生成

| 区分 | 時間範囲 | 出現確率 |
|---|---|---|
| Short | 15 〜 60 分 | 70% |
| Medium | 60 〜 120 分 | 20% |
| Long | 120 〜 180 分 | 10% |

| 重要度 | スコア係数 | 出現確率 |
|---|---|---|
| LOW | 1 | 60% |
| MEDIUM | 2 | 25% |
| HIGH | 3 | 15% |

ジャンルは 4 種類（均等分布）で付与される。

### 集中力モデル

| パラメータ | 値 |
|---|---|
| 初期レベル | 1.0 |
| 最低レベル | 0.2 |
| 連続作業上限 | 120 分 |
| 休憩回復時間 | 15 分 |
| 休憩閾値 | 0.4 |

集中力レベルによって作業効率が変わる。

| 条件 | 効率倍率 |
|---|---|
| 高集中（レベル > 0.7） | 0.8 倍（短縮） |
| 低集中 | 1.2 倍（遅延） |

優先度ごとに最適な集中力レベルがある。

| 優先度 | 最適集中力レベル |
|---|---|
| LOW | ≥ 0.3 |
| MEDIUM | ≥ 0.6 |
| HIGH | ≥ 0.8 |

タスクの実行順によるジャンル切り替え効果もある。`personal_data.json` で設定された好向性タイプに応じ、同じジャンルを続けるか切り替えるかで集中力に影響が出る。

| 好向性タイプ | 同じジャンル続行 | ジャンル切り替え |
|---|---|---|
| same（現在の設定） | +0.05 ボーナス | ペナルティなし |
| switch | ペナルティなし | +0.05 ボーナス |

### 強化学習

| パラメータ | 値 |
|---|---|
| 学習率 | 0.1 |
| 割引率 | 0.9 |
| ε（テスト時） | 0.0 |
| ε（学習時） | 0.5（decay 0.995、下限 0.05） |

**報酬設計:**

| 報酬項目 | 値 |
|---|---|
| 高集中完了ボーナス（レベル > 0.7） | +20 |
| 高重要度完了ボーナス（重要度 > 0.8） | +30 |
| 失敗時ペナルティ | ×0.5 |
| 無謀実行ペナルティ（集中力 < 0.6） | -50 |

**状態空間の離散化:**

| 次元 | 区間数 |
|---|---|
| 残りタスク数 | 10 段階 |
| 高重要度比率 | 5 段階 |
| 締切urgency | 10 段階（12 時間刻み） |
| 平均所要時間 | 5 段階（20 分刻み） |
| 集中力レベル | 4 段階 |
| 疲労蓄積度 | 5 段階 |

### スコア

`スコア = Σ(完了タスクの基本所要時間 × 重要度)` （重要度：LOW=1, MEDIUM=2, HIGH=3）

## プロジェクト構成

```
.
├── config.py                        # 全設定の一元管理
├── run_full_experiment.py           # 実験実行エントリーポイント
├── train_rl_model.py                # RL モデルの事前学習
├── generate_task_dataset.py         # タスクデータセット生成
├── src/
│   ├── models/                      # データモデル（Task, ConcentrationModel）
│   ├── environment/                 # シミュレーション環境
│   ├── schedulers/                  # 4つのスケジューラー実装
│   ├── evaluation/                  # 実験評価・統計テスト
│   ├── visualization/               # ガンツチャート等の可視化
│   └── utils/                       # タスクローダー・ファクトリー
├── dataset/                         # 事前生成したタスクデータ
├── trained_models/                  # 学習済み RL モデル
├── results/                         # 実験結果・画像の出力先
└── tests/                           # テスト
```

## 実行方法

```bash
# 1. タスクデータセットを生成
python generate_task_dataset.py

# 2. RL モデルを事前学習
python train_rl_model.py

# 3. 実験実行（レポート・グラフ一体）
python run_full_experiment.py
```

## 実験出力

`run_full_experiment.py` の実行で以下が生成される。

- **統計レポート**: 各手法のスコア・完了率・有意差検定結果
- **スケジュール比較グラフ**: 4手法のスケジュールを並べたガンツチャート
- **箱ひげ図**: スコアと完了率の分布を可視化
- **週次比較**: 3週間分のスケジュール比較を生成し、RL の Q-table が週を通じて継続学習していく過程を可視化

出力先: `results/{実行日時}/week_{1,2,3}/`
