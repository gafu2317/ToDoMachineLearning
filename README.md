# ToDoMachineLearning
知能プログラミング演習グループ課題

機械学習（強化学習）と単純なルールベース手法によるタスクスケジューリング性能の比較検証システム。

## 概要
個人の集中力変動とタスク難易度を考慮した環境で、複数のタスクスケジューリング手法を比較し、**個人適応型スケジューリング**の有効性を検証する。

### 特徴的な設計
本システムは「タスク中心」ではなく「**個人中心**」のスケジューリングを実現する：
- タスクには難易度（1-3）が設定され、実行には一定の集中力が必要
- 集中力不足での実行は失敗確率が高く、時間を無駄にする
- 強化学習は個人の集中力状態に応じて「今実行できるタスク」を選択することを学習
- 単純な優先度順スケジューラは集中力を考慮せず、失敗を繰り返す

## プロジェクト構成
```
├── src/                    # 核となるライブラリ
│   ├── models/            # タスク・集中力モデル
│   ├── schedulers/        # スケジューリング手法
│   ├── environment/       # シミュレーション環境
│   ├── evaluation/        # 評価・分析システム
│   └── utils/            # 共通ユーティリティ
├── trained_models/       # 学習済みモデル保存先
├── results/              # 実験結果ファイル
├── config.py            # 設定ファイル
├── requirements.txt     # 依存関係
├── train_rl_model.py           # 強化学習モデル事前学習
├── generate_detailed_log.py     # 詳細ログ生成
└── run_full_experiment.py      # 本格実験実行
```

## 実装済み機能
- ✅ 複数スケジューリング手法（期限順・重要度順・ランダム・強化学習）
- ✅ 集中力変動モデル（指数減衰・休憩回復）
- ✅ **タスク難易度システム**（難易度1-3、集中力要件、成功確率モデル）
- ✅ **個人適応型強化学習**（集中力マッチングポリシー）
- ✅ 失敗・再実行メカニズム
- ✅ 統計的有意差検定（t検定）
- ✅ 詳細ログ出力・分析（失敗回数・難易度表示）
- ✅ 公平な比較環境（全手法で同一タスクセット使用）

## 使用方法

### 1. 環境構築
```bash
# 依存関係インストール
pip install -r requirements.txt
```

### 2. 強化学習モデルの事前学習（初回のみ）
```bash
# 1000エピソードで学習（5-10分程度）
python train_rl_model.py
```
学習済みモデルは `trained_models/rl_model_default.pkl` に保存される。

**学習内容：**
- 7つの選択ポリシーを状況に応じて使い分けることを学習
- 集中力レベルとタスク難易度のマッチングを学習
- 失敗ペナルティから「無理なタスクを避ける」戦略を獲得

### 3. 実験実行
```bash
# 詳細ログ付き比較実験（1回のシミュレーション）
python generate_detailed_log.py

# 本格統計実験（50回 × 4手法）
python run_full_experiment.py
```

## 実験結果

### 個人適応型強化学習の優位性を実証

**最新実験結果（50回試行の平均）：**

| 手法 | スコア | 完了率 | 失敗回数（例） |
|------|--------|--------|---------------|
| **強化学習（個人適応）** | **4388.70** | **76.7% ★** | 7回 |
| 重要度順 | 4754.54 ★ | 56.2% | 19回 |
| 期限順 | 3044.28 | 69.0% | - |
| ランダム | 3771.36 | 53.4% | - |

**主要な発見：**
1. **完了率で圧倒的優位**：強化学習が76.7%で1位（重要度順+20.5%）
2. **失敗回数の大幅削減**：強化学習7回 vs 重要度順19回（63%削減）
3. **スコアでも健闘**：重要度順の92%を達成（完了率とのトレードオフを最適化）
4. **統計的有意差**：全手法との比較でp<0.001（高い信頼性）

### なぜ強化学習が優れているのか

**重要度順スケジューラの問題：**
- 重要＝難しいタスクを優先
- 集中力が低い状態でも難易度3のタスクを実行
- 失敗→再実行を繰り返し、時間を浪費
- 結果：高スコアだが完了率が低い

**強化学習スケジューラの戦略：**
- 集中力状態を常に監視
- 「今の自分に合ったタスク」を選択
- 集中力が低い時は簡単なタスクで確実に進める
- 集中力が高い時に難しいタスクに挑戦
- 結果：失敗が少なく、完了率が高い

### システムの核心設計

#### 1. タスク難易度システム
```
難易度1（簡単）：集中力30%以上で成功
難易度2（普通）：集中力60%以上で成功
難易度3（難しい）：集中力80%以上で成功
```
重要度が高いタスクほど難易度が高い傾向（相関あり）

#### 2. 失敗メカニズム
- 集中力不足でタスクを実行すると確率的に失敗
- 失敗しても時間は消費される（時間の無駄）
- 失敗したタスクは再挑戦が必要
- 失敗ペナルティで強化学習が「無理をしない」戦略を学習

#### 3. 強化学習の7つのポリシー
```
0. highest_priority      - 最も重要度が高いタスク
1. nearest_deadline      - 最も締切が近いタスク
2. shortest_task         - 最も短時間で終わるタスク
3. highest_score         - 最もスコアが高いタスク
4. priority_deadline_mix - 重要度と締切のバランス
5. concentration_matched - 集中力に合った難易度のタスク ★
6. safe_high_priority    - 成功確率が高い重要タスク ★
```
★印が個人適応型の新ポリシー

#### 4. 状態空間（Q-learning）
```python
(タスク数, 重要度分布, 締切緊急度, 平均時間, 集中力レベル)
```
集中力レベルを状態に含めることで、状況に応じた最適行動を学習

#### 5. 報酬設計
```
成功時：タスクスコア + 高集中ボーナス + 難易度ボーナス
失敗時：-作業時間×0.5 - 無謀な挑戦ペナルティ(-50)
```

## 技術的詳細

詳細な結果は`results/`フォルダを参照。

**主要ファイル：**
- [src/models/task.py](src/models/task.py) - 難易度・成功確率モデル
- [src/schedulers/rl_policy_selector.py](src/schedulers/rl_policy_selector.py) - 個人適応型ポリシー
- [src/schedulers/scheduler.py](src/schedulers/scheduler.py) - 失敗判定ロジック
- [src/environment/simulation.py](src/environment/simulation.py) - 失敗統計収集
