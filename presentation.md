---
marp: true
theme: default
paginate: true
---

# Todoリストの優先順位における強化学習と締切優先等の比較

---

## テーマと目標

**テーマ**
タスクをこなす際の順序について期限順や優先度順と強化学習の提示する順序でどれがよりタスクを効率的にこなせるかの比較検証

**目標**
強化学習の優先順位出力と、一つの要素のみを考慮したTodoリストの比較により、強化学習の有用性を確認したい

---

## 仮説

単純な条件の優先順位より、強化学習を用いた優先順位の方が有用なのではないか

LLMで抽出したペルソナ情報を使って強化学習すれば締切順や重要度順よりも良いスケジューラができるのではないか

---

## システム概要

**主要コンポーネント**
- **タスク管理システム**: 40個のタスク、3段階の重要度、期限設定
- **集中力モデル**: 疲労蓄積、休憩効果、重要度による疲労増加を考慮
- **4種類のスケジューラ**: 期限順、重要度順、ランダム、強化学習
- **シミュレーション環境**: 7日間、1日8時間の作業時間

**実験設定**
- 実験回数: 50回
- シミュレーション期間: 7日間
- 1日の作業時間: 8時間

---

## システム構成図


---

## LLMの使用

**ペルソナ作成**
- 集中力の変化しやすさ
- 得意不得意によるタスク時間算出
- ワークスタイルによるジャンル切り替え

**Todoリストの正規化**
- 得意不得意によるタスク所要時間の調整
- それぞれのタスクへのジャンル付け

---

## 機械学習モデル

**Q-learning (強化学習)**

**状態空間**
- タスク数の区分（10個ごと、最大10区間）
- 重要度比率の区分（5区間）
- 締切の区分（12時間ごと、最大10区間）
- 平均時間の区分（30分ごと、最大7区間）
- 集中力レベル（4区間）
- 疲労蓄積度（5区間）

**行動空間**
どのタスクを実行するか、休憩を取るか

---

## 機械学習モデル (続き)

**報酬関数**
- 高集中完了ボーナス: +20点（集中力0.7以上）
- 高重要度ボーナス: +150点（重要度HIGH完了時）
- 失敗時間ペナルティ: -実際の時間 × 0.5
- 無謀なタスク挑戦ペナルティ: -20点

**学習パラメータ**
- 学習率: 0.1
- 割引率: 0.9
- 探索率(ε): 学習時0.5、評価時0.0

---

## 評価指標

**スコア**
```
スコア = 重要度 × 基礎時間
重要度: 低=1, 中=2, 高=3
```

**タスク完了率**
どのくらいのタスクを完了できたか

---

## 結果 - スケジューラ別比較

| スケジューラ | 平均スコア | 完了率 | 締切遵守率 | 効率 |
|------------|----------|--------|----------|------|
| 期限順 | 4376.36 ± 381.36 | **90.4%** | **39.7%** | 91.7% |
| 重要度順 | **5843.72 ± 682.44** | 61.6% | 22.3% | 93.0% |
| ランダム | 4932.22 ± 440.93 | 76.8% | 30.5% | **93.2%** |
| 強化学習 | 4398.52 ± 322.85 | 90.0% | 39.5% | 92.1% |

**最良の指標**
- **スコア最大**: 重要度順（5843.72点）
- **完了率最大**: 期限順（90.4%）
- **締切遵守率最大**: 期限順（39.7%）

---

## 結果 - 統計的有意差

**有意差あり (p < 0.05)**
- 期限順 vs 重要度順: p=0.0000
- 期限順 vs ランダム: p=0.0000
- 重要度順 vs ランダム: p=0.0000
- 重要度順 vs 強化学習: p=0.0000
- ランダム vs 強化学習: p=0.0000

**有意差なし**
- **期限順 vs 強化学習**: p=0.7545

→ 期限順と強化学習のパフォーマンスは統計的に同等

---

## 考察

**重要度順スケジューラの高スコア**
- 重要度の高いタスク（高得点）を優先的に完了
- しかし完了率は61.6%と低い
- 重要度が高いタスクは長時間かつ疲労が大きく、期限遵守が困難

**期限順スケジューラの安定性**
- 完了率90.4%と高い
- 締切遵守率も最高の39.7%
- バランスの取れた結果を実現

**強化学習の課題**
- 期限順と統計的に同等の結果（p=0.7545）
- 仮説「強化学習が最良」は実証されなかった
- 可能性のある原因:
  - 状態空間の離散化が粗い
  - 報酬設計が最適でない
  - 学習データ不足

---

## 考察 (続き)

**ランダムスケジューラの意外な性能**
- スコア4932.22点で3番目
- 効率93.2%と最高
- 集中力や疲労のバランスが偶然取れた可能性

**システムの複雑性**
- 集中力モデル、疲労蓄積、重要度による疲労増加など複雑な要因
- 単純な戦略（期限順）でも十分な性能
- 強化学習の優位性を示すには、より複雑なタスク設定が必要

**今後の改善方向**
1. 状態空間の細分化
2. 報酬関数の再設計（完了率や締切遵守を重視）
3. 学習エピソード数の増加
4. より複雑なタスク依存関係の導入

---

## まとめ

**検証結果**
- 重要度順: 高スコアだが完了率が低い
- 期限順: 最もバランスの取れた結果
- 強化学習: 期限順と同等だが、仮説通りの優位性は見られず

**得られた知見**
- 単純な戦略でも効果的なスケジューリングが可能
- 集中力や疲労を考慮したモデルの重要性
- 強化学習の改善余地（状態空間、報酬設計）

**今後の展望**
- モデルの改良と追加学習
- より複雑なタスク環境での検証
- 実際のユーザー行動データの活用
