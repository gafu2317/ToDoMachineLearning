---
marp: true
theme: default
paginate: true
---

# Todoリストの優先順位における強化学習と締切優先等の比較

---

## テーマと目標

**テーマ**
タスクをこなす際の順序について期限順や優先度順と強化学習の提示する順序でどれがよりタスクを効率的にこなせるかの比較検証

**目標**
強化学習の優先順位出力と、一つの要素のみを考慮したTodoリストの比較により、強化学習の有用性を確認したい

---

## 仮説

単純な条件の優先順位より、強化学習を用いた優先順位の方が有用なのではないか

LLMで抽出したペルソナ情報を使って強化学習すれば締切順や重要度順よりも良いスケジューラができるのではないか

---

## システム概要

**主要コンポーネント**
- **タスク管理システム**: 60個のタスク、3段階の重要度、期限設定（2-7日）
- **集中力モデル**: 疲労蓄積、休憩効果、重要度による疲労増加を考慮
- **4種類のスケジューラ**: 期限順、重要度順、ランダム、強化学習
- **シミュレーション環境**: 7日間、1日8時間の作業時間

**実験設定**
- 実験回数: 50回
- シミュレーション期間: 7日間
- 1日の作業時間: 8時間
- タスク数: 60個

---

## システム構成図


---

## LLMの使用

**ペルソナ作成**
- 集中力の変化しやすさ
- 得意不得意によるタスク時間算出
- ワークスタイルによるジャンル切り替え

**Todoリストの正規化**
- 得意不得意によるタスク所要時間の調整
- それぞれのタスクへのジャンル付け

---

## 機械学習モデル

**Q-learning (強化学習)**

**状態空間**
- タスク数の区分（15個ごと、最大5区間）
- 重要度比率の区分（3区間）
- 締切の区分（24時間ごと、最大5区間）
- 平均時間の区分（60分ごと、最大4区間）
- 集中力レベル（3区間）
- 疲労蓄積度（3区間）
- 残り日数（5区間）

**行動空間**
どのタスクを実行するか、休憩を取るか

---

## 機械学習モデル (続き)

**報酬関数**
- 高集中完了ボーナス: +50点（集中力0.7以上）
- 高重要度ボーナス: +100点（重要度HIGH完了時）
- 締切遵守ボーナス: +50点
- 締切違反ペナルティ: -200点
- 早期完了ボーナス: +30点（締切24時間前完了時）
- 失敗時間ペナルティ: -実際の時間 × 1.0
- 無謀なタスク挑戦ペナルティ: -80点
- 長時間タスク早期完了ボーナス: +80点

**学習パラメータ**
- 学習率: 0.05
- 割引率: 0.95
- 探索率(ε): 学習時0.5、評価時0.0
- 学習エピソード数: 10,000

---

## 評価指標

**スコア**
```
スコア = 重要度 × 基礎時間
重要度: 低=1, 中=2, 高=3
```

**タスク完了率**
どのくらいのタスクを完了できたか

---

## 結果 - スケジューラ別比較

| スケジューラ | 平均スコア | 完了率 | 締切遵守率 | 効率 |
|------------|----------|--------|----------|------|
| 期限順 | 4850.66 ± 283.66 | 83.3% | 83.3% | 91.8% |
| 重要度順 | **5060.78 ± 359.49** | 62.7% | 62.7% | 91.5% |
| ランダム | 4807.98 ± 320.21 | **84.4%** | **84.4%** | **91.9%** |
| 強化学習 | 4784.80 ± 300.05 | 77.8% | 77.8% | 91.2% |

**最良の指標**
- **スコア最大**: 重要度順（5060.78点）
- **完了率最大**: ランダム（84.4%）
- **締切遵守率最大**: ランダム（84.4%）

---

## 結果 - 統計的有意差

**有意差あり (p < 0.05)**
- 期限順 vs 重要度順: p=0.0016 **
- 重要度順 vs ランダム: p=0.0003 **
- 重要度順 vs 強化学習: p=0.0001 **

**有意差なし**
- 期限順 vs ランダム: p=0.4822
- **期限順 vs 強化学習**: p=0.2621
- **ランダム vs 強化学習**: p=0.7096

→ 期限順、ランダム、強化学習の3つは統計的に同等のパフォーマンス

---

## 考察

**重要度順スケジューラの高スコア**
- 重要度の高いタスク（高得点）を優先的に完了
- しかし完了率は62.7%と低い
- 重要度が高いタスクは長時間かつ疲労が大きく、期限遵守が困難

**ランダムスケジューラの意外な高パフォーマンス**
- 完了率・締切遵守率が最高の84.4%
- 期限順、強化学習と統計的に同等（p>0.2）
- 無作為な選択が結果的にバランス良くタスクをこなした

**強化学習の現状**
- 期限順・ランダムと統計的に同等の結果
- 仮説「強化学習が最良」は実証されなかった
- 可能性のある原因:
  - 状態空間の離散化を粗くしすぎた（細かい差を捉えられない）
  - 報酬設計が複雑すぎて最適化が難しい
  - 学習エピソード数10,000でも不足している可能性

---

## 考察 (続き)

**タスク数増加の影響（40→60個）**
- タスク数を増やしたことで、スケジューラ間の差が縮小
- 完了率・締切遵守率が全体的に向上（より現実的な負荷）
- 期限設定を2-7日に変更したことで、早めの締切タスクが増加

**システムの複雑性**
- 集中力モデル、疲労蓄積、重要度による疲労増加など複雑な要因
- 単純な戦略（期限順、ランダム）でも十分な性能
- 強化学習の優位性を示すには、より複雑なタスク設定が必要

**今後の改善方向**
1. 状態空間の再設計（離散化を細かくするか、関数近似を導入）
2. 報酬関数のシンプル化（複雑すぎると学習が困難）
3. 学習エピソード数の大幅増加（10,000→50,000以上）
4. ディープQ学習など、より高度な手法の導入

---

## まとめ

**検証結果**
- 重要度順: 高スコアだが完了率が低い（62.7%）
- ランダム: 完了率・締切遵守率が最高（84.4%）
- 期限順・強化学習: ランダムと統計的に同等
- 強化学習: 仮説通りの優位性は見られず

**得られた知見**
- 単純な戦略（期限順、ランダム）でも効果的なスケジューリングが可能
- タスク数60個、締切2-7日の設定では差が縮小
- 集中力や疲労を考慮したモデルの重要性
- 強化学習の改善余地が大きい（状態空間、報酬設計、学習量）

**今後の展望**
- 状態空間の再設計と学習量の増加
- より高度な強化学習手法（DQNなど）の導入
- より複雑なタスク環境での検証
- 実際のユーザー行動データの活用
