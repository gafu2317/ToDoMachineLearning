"""
ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼æ¯”è¼ƒãƒ†ã‚¹ãƒˆ
å°‘ãªã„å®Ÿé¨“å›æ•°ã§3ã¤ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’æ¯”è¼ƒã™ã‚‹
"""

import sys
import os

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’è¿½åŠ 
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.evaluation.evaluator import SchedulerEvaluator


def main():
    """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼æ¯”è¼ƒãƒ†ã‚¹ãƒˆ"""
    print("ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼æ¯”è¼ƒãƒ†ã‚¹ãƒˆã‚’é–‹å§‹...")
    print()
    
    # å°‘ãªã„å®Ÿé¨“å›æ•°ã§ãƒ†ã‚¹ãƒˆï¼ˆæ™‚é–“çŸ­ç¸®ã®ãŸã‚ï¼‰
    evaluator = SchedulerEvaluator(
        num_experiments=10,  # 10å›ã ã‘
        simulation_days=2,   # 2æ—¥é–“
        work_hours_per_day=6, # 6æ™‚é–“
        num_tasks=20         # 20å€‹ã®ã‚¿ã‚¹ã‚¯
    )
    
    print("å®Ÿé¨“ã‚’å®Ÿè¡Œä¸­...")
    results_df = evaluator.run_experiments()
    
    print(f"\nå®Ÿé¨“å®Œäº†ï¼ {len(results_df)} ä»¶ã®çµæœã‚’å–å¾—")
    print()
    
    # çµæœåˆ†æ
    analysis = evaluator.analyze_results(results_df)
    
    print("=== ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼åˆ¥çµæœ ===")
    for scheduler_name, stats in analysis.items():
        if scheduler_name == 'summary':
            continue
        
        print(f"\n{scheduler_name}:")
        print(f"  å¹³å‡ã‚¹ã‚³ã‚¢: {stats['mean_score']:.2f} Â± {stats['std_score']:.2f}")
        print(f"  å®Œäº†ç‡: {stats['mean_completion_rate']:.3f} Â± {stats['std_completion_rate']:.3f}")
        print(f"  ç· åˆ‡éµå®ˆç‡: {stats['mean_deadline_compliance']:.3f} Â± {stats['std_deadline_compliance']:.3f}")
        print(f"  åŠ¹ç‡: {stats['mean_efficiency']:.3f} Â± {stats['std_efficiency']:.3f}")
    
    print(f"\n=== ç·åˆçµæœ ===")
    summary = analysis['summary']
    print(f"ã‚¹ã‚³ã‚¢ãŒæœ€ã‚‚é«˜ã„: {summary['best_scheduler_by_score']}")
    print(f"å®Œäº†ç‡ãŒæœ€ã‚‚é«˜ã„: {summary['best_scheduler_by_completion']}")
    print(f"ç· åˆ‡éµå®ˆç‡ãŒæœ€ã‚‚é«˜ã„: {summary['best_scheduler_by_deadline']}")
    
    # çµ±è¨ˆçš„æœ‰æ„å·®æ¤œå®š
    print(f"\n=== çµ±è¨ˆçš„æœ‰æ„å·®æ¤œå®š ===")
    significance = evaluator.statistical_significance_test(results_df)
    
    for comparison, test_result in significance.items():
        significance_mark = "**æœ‰æ„å·®ã‚ã‚Š**" if test_result['significant'] else "æœ‰æ„å·®ãªã—"
        print(f"{comparison}:")
        print(f"  på€¤: {test_result['p_value']:.4f} ({significance_mark})")
        print(f"  å¹³å‡å·®: {test_result['mean_diff']:.2f}")
    
    # è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º
    print(f"\n=== è©³ç´°ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚¹ã‚³ã‚¢ï¼‰ ===")
    for scheduler_name in results_df['scheduler_name'].unique():
        scores = results_df[results_df['scheduler_name'] == scheduler_name]['total_score']
        print(f"{scheduler_name}: {scores.tolist()}")
    
    print("\nğŸ‰ æ¯”è¼ƒãƒ†ã‚¹ãƒˆå®Œäº†ï¼")


if __name__ == "__main__":
    main()