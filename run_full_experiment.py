"""
å¼·åŒ–å­¦ç¿’ã‚’å«ã‚€æœ¬æ ¼å®Ÿé¨“å®Ÿè¡Œ
å…¨çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹
"""

import sys
import os
from datetime import datetime

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’è¿½åŠ 
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.evaluation.evaluator import SchedulerEvaluator
from src.utils.task_loader import TaskDataLoader
from src.visualization.result_plotter import ResultPlotter
from config import DEFAULT_SIMULATION_CONFIG, EXPERIMENT_CONFIG


def main():
    """å¼·åŒ–å­¦ç¿’ã‚’å«ã‚€æœ¬æ ¼å®Ÿé¨“ã‚’å®Ÿè¡Œã—ã¦ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    print("å¼·åŒ–å­¦ç¿’ã‚’å«ã‚€æœ¬æ ¼å®Ÿé¨“ã‚’é–‹å§‹...")

    # ã‚¿ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
    test_loader = TaskDataLoader(dataset_type='test')

    # å®Ÿé¨“è¨­å®š
    evaluator = SchedulerEvaluator(
        num_experiments=EXPERIMENT_CONFIG['num_experiments'],
        task_loader=test_loader,
        **DEFAULT_SIMULATION_CONFIG
    )

    print("å®Ÿé¨“å®Ÿè¡Œä¸­... (å¼·åŒ–å­¦ç¿’å«ã‚€ãŸã‚æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™)")
    results_df = evaluator.run_experiments()
    
    # çµæœã‚’ä¿å­˜
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã«è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    csv_path = f"{EXPERIMENT_CONFIG['output_dir']}/full_experiment_results_{timestamp}.csv"
    os.makedirs(EXPERIMENT_CONFIG['output_dir'], exist_ok=True)
    results_df.to_csv(csv_path, index=False, encoding='utf-8')
    print(f"è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜: {csv_path}")
    
    # ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆãƒ»ä¿å­˜
    report_path = f"{EXPERIMENT_CONFIG['output_dir']}/full_experiment_report_{timestamp}.md"
    report = evaluator.generate_report(results_df, save_path=report_path)
    print(f"ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {report_path}")
    
    # å¼·åŒ–å­¦ç¿’ã®è©³ç´°åˆ†æ
    rl_analysis_path = f"{EXPERIMENT_CONFIG['output_dir']}/rl_analysis_{timestamp}.txt"
    with open(rl_analysis_path, 'w', encoding='utf-8') as f:
        f.write("# å¼·åŒ–å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è©³ç´°åˆ†æ\n\n")
        
        # å¼·åŒ–å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
        rl_data = results_df[results_df['scheduler_name'] == 'rl_scheduler']
        
        f.write(f"å®Ÿé¨“å›æ•°: {len(rl_data)}\n")
        f.write(f"å¹³å‡ã‚¹ã‚³ã‚¢: {rl_data['total_score'].mean():.2f}\n")
        f.write(f"æœ€é«˜ã‚¹ã‚³ã‚¢: {rl_data['total_score'].max():.2f}\n")
        f.write(f"æœ€ä½ã‚¹ã‚³ã‚¢: {rl_data['total_score'].min():.2f}\n")
        f.write(f"æ¨™æº–åå·®: {rl_data['total_score'].std():.2f}\n\n")
        
        f.write("å„å®Ÿé¨“ã®è©³ç´°çµæœ:\n")
        for idx, row in rl_data.iterrows():
            f.write(f"å®Ÿé¨“{row['experiment_id']}: ")
            f.write(f"ã‚¹ã‚³ã‚¢={row['total_score']:.0f}, ")
            f.write(f"å®Œäº†ç‡={row['completion_rate']:.3f}, ")
            f.write(f"åŠ¹ç‡={row['efficiency']:.3f}\n")
    
    print(f"å¼·åŒ–å­¦ç¿’è©³ç´°åˆ†æã‚’ä¿å­˜: {rl_analysis_path}")
    
    # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
    print("\n" + "="*60)
    print("å®Ÿé¨“ã‚µãƒãƒªãƒ¼")
    print("="*60)
    
    analysis = evaluator.analyze_results(results_df)
    
    for scheduler_name, stats in analysis.items():
        if scheduler_name == 'summary':
            continue
        
        print(f"\n{scheduler_name}:")
        print(f"  å¹³å‡ã‚¹ã‚³ã‚¢: {stats['mean_score']:.2f} Â± {stats['std_score']:.2f}")
        print(f"  å®Œäº†ç‡: {stats['mean_completion_rate']:.3f} Â± {stats['std_completion_rate']:.3f}")
    
    print(f"\n=== ç·åˆçµæœ ===")
    summary = analysis['summary']
    print(f"ã‚¹ã‚³ã‚¢ãŒæœ€ã‚‚é«˜ã„: {summary['best_scheduler_by_score']}")
    print(f"å®Œäº†ç‡ãŒæœ€ã‚‚é«˜ã„: {summary['best_scheduler_by_completion']}")
    
    # çµ±è¨ˆçš„æœ‰æ„å·®æ¤œå®š
    print(f"\n=== çµ±è¨ˆçš„æœ‰æ„å·®æ¤œå®š ===")
    significance = evaluator.statistical_significance_test(results_df)

    rl_comparisons = {k: v for k, v in significance.items() if 'rl_scheduler' in k}
    for comparison, test_result in rl_comparisons.items():
        significance_mark = "**æœ‰æ„å·®ã‚ã‚Š**" if test_result['significant'] else "æœ‰æ„å·®ãªã—"
        print(f"{comparison}: p={test_result['p_value']:.4f} ({significance_mark})")

    # è¦–è¦šåŒ–
    print(f"\n=== ã‚°ãƒ©ãƒ•ç”Ÿæˆä¸­ ===")
    plotter = ResultPlotter(results_df, output_dir=EXPERIMENT_CONFIG['output_dir'])

    # å€‹åˆ¥ã‚°ãƒ©ãƒ•ç”Ÿæˆ
    score_graph_path = f"{EXPERIMENT_CONFIG['output_dir']}/score_comparison_{timestamp}.png"
    plotter.plot_score_comparison(save_path=score_graph_path)

    metrics_graph_path = f"{EXPERIMENT_CONFIG['output_dir']}/metrics_comparison_{timestamp}.png"
    plotter.plot_metrics_comparison(save_path=metrics_graph_path)

    distribution_graph_path = f"{EXPERIMENT_CONFIG['output_dir']}/score_distribution_{timestamp}.png"
    plotter.plot_score_distribution(save_path=distribution_graph_path)

    significance_graph_path = f"{EXPERIMENT_CONFIG['output_dir']}/statistical_significance_{timestamp}.png"
    plotter.plot_statistical_significance(significance, save_path=significance_graph_path)

    # ç·åˆãƒ¬ãƒãƒ¼ãƒˆï¼ˆPDFï¼‰
    plotter.create_comprehensive_report(significance, timestamp)

    print(f"\nâœ… å®Ÿé¨“å®Œäº†ï¼çµæœã¯ä»¥ä¸‹ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ:")
    print(f"  - è©³ç´°ãƒ‡ãƒ¼ã‚¿: {csv_path}")
    print(f"  - ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")
    print(f"  - å¼·åŒ–å­¦ç¿’åˆ†æ: {rl_analysis_path}")
    print(f"\nğŸ“Š ã‚°ãƒ©ãƒ•:")
    print(f"  - ã‚¹ã‚³ã‚¢æ¯”è¼ƒ: {score_graph_path}")
    print(f"  - å„æŒ‡æ¨™æ¯”è¼ƒ: {metrics_graph_path}")
    print(f"  - ã‚¹ã‚³ã‚¢åˆ†å¸ƒ: {distribution_graph_path}")
    print(f"  - çµ±è¨ˆçš„æœ‰æ„å·®: {significance_graph_path}")
    print(f"  - ç·åˆãƒ¬ãƒãƒ¼ãƒˆï¼ˆPDFï¼‰: {EXPERIMENT_CONFIG['output_dir']}/comprehensive_report_{timestamp}.pdf")


if __name__ == "__main__":
    main()